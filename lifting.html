<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Evaluation of Learning Algorithms on Weight-lifting Activity Data</title>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}

pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Evaluation of Learning Algorithms on Weight-lifting Activity Data</h1>

<h2>Abstract</h2>

<p>This report covers the application of several learning algorithms to classify
weight-lifting activity based on measurements from wearable sensors.</p>

<p>The dataset
was taken from an experiment performed by <a href="http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201">Velloso et al</a>, in which study
participants lifted a dumbbell either correctly or with one of four common
mistakes. Measurements were taken from sensors on the dumbbell and on the
participants&#39; upper arm, forearm, and belt.</p>

<p>Three learning algorithms were evaluated on this dataset: a random forest,
a neural network, and a na&iuml;ve Bayesian classifier. The random forest
was found to offer the strongest performance, with over 99% accuracy on the
training set and and estimated out-of-sample error rate of around 0.01.</p>

<h2>Data Acquisition and Preprocessing</h2>

<p>The dataset was acquired from a mirror, already separated into training and
test sets consisting respectively of 19622 and
20 observations.</p>

<p>The complete dataset contains 160 variables; this includes
the classification outcome as well as identifiers that should not be used as
predictors, such as the subject&#39;s first name. These were discarded from both
the training and test sets during pre-processing.</p>

<p>Among the remaining variables, many had missing values. Many of variables
with missing values were aggregate functions (like maximum and standard
deviation) taken over the entire time period during which an activity was
performed. These were removed for two reasons. First, while the original
research had used a sliding window technique to evaluate multiple
sequential observations, this analysis considers each observation independently;
indeed, the 160 observations used as the test set are
each taken from a different window and contain no aggregate values. Second,
it was believed that the aggregate functions, being derived from other
predictors, could be removed without loss of information.</p>

<p>Ultimately, only variables with no missing values in either the training
or test set were retained.</p>

<pre><code class="r">mkprocdataset &lt;- function(getfn) {
    function () {
        dat &lt;- getfn()
        # remove columns that are not predictors
        rownames(dat) &lt;- dat$X
        dat$X &lt;- NULL
        dat$user_name &lt;- NULL
        # the assignment appears to require ignoring these time series-related
        # variables and treating each observation independently
        dat$raw_timestamp_part_1 &lt;- NULL
        dat$raw_timestamp_part_2 &lt;- NULL
        dat$cvtd_timestamp &lt;- NULL
        dat$new_window &lt;- NULL
        dat$num_window &lt;- NULL
        dat
    }
}
gettrainset &lt;- mkprocdataset(gettrain.orig)
gettestset &lt;- mkprocdataset(gettest.orig)
sapplycols &lt;- function(dat, fn) {
    sapply(colnames(dat), function(colname) { fn(dat[,colname]) })
}
iscomplete &lt;- function(v) {
    !any(is.na(v))
}
completecols &lt;- function(dat) {
    sapplycols(dat, iscomplete)
}
mkcommonsubset &lt;- function(gettrain, gettest) {
    function () {
        trainset &lt;- gettrain()
        testset &lt;- gettest()        
        completecols &lt;- intersect(which(completecols(trainset)),
                                  which(completecols(testset)))
        list(training=trainset[,completecols],
             testing=testset[,completecols])
    }
}
workingsets &lt;- mkcommonsubset(gettrainset, gettestset)
data &lt;- workingsets()
</code></pre>

<p>Finally, the measurements were normalized before being used for training:</p>

<pre><code class="r">mkpreproc &lt;- function(dat, ...) {
    preproc &lt;- preProcess(dat[,ispredictor(dat)], ...)
    function(dat) {
        ndat &lt;- dat
        ndat[,ispredictor(dat)] &lt;- predict(preproc, dat[,ispredictor(dat)])
        ndat
    }
}
</code></pre>

<h2>Learning Algorithm Evaluation</h2>

<p>Three learning algorithms were evaluated on this dataset: a random forest,
a neural network, and a na&iuml;ve Bayesian classifier. These
algorithms were chosen based on their performance in a learning
algorithm bake-off presented by <a href="http://www.cs.cornell.edu/%7Ecaruana/ctp/ct.papers/caruana.icml06.pdf">Caruana and Niculescu-Mizil</a> in 2006.
In those results, random forests and neural networks were found to perform
very strongly on a variety of datasets, while na&iuml;ve Bayesian classifiers
were found to have very weak overall performance.</p>

<p>The implementations chosen were all provided by the <code>caret</code> library in R.
The <code>parRF</code> method was used for a random forest, <code>nnet</code> for a neural network,
and <code>nb</code> for a na&iuml;ve Bayesian classifier.</p>

<pre><code class="r">candidate.algorithms &lt;- c(&quot;parRF&quot;, &quot;nnet&quot;, &quot;nb&quot;)
</code></pre>

<pre><code class="r">isoutcome &lt;- function(dat) {
    colnames(dat) %in% c(&quot;classe&quot;, &quot;problem_id&quot;)
}
ispredictor &lt;- function(dat) {
    !isoutcome(dat)
}
mktrain &lt;- function(dat) {
    function(algname) {
        train(dat[,ispredictor(dat)], dat[,isoutcome(dat)], method=algname,
              trControl=trainControl(method=&quot;cv&quot;, number=4))
    }
}
mkconfusion &lt;- function(dat) {
    function(model) {
        predictions &lt;- predict(model, dat)
        confusionMatrix(predictions, dat[,isoutcome(dat)])
    }
}
mktimer &lt;- function(fn) {
    function(...) {
        t &lt;- system.time(res &lt;- fn(...))
        list(t=t, res=res)
    }
}
timer.getresults &lt;- function(l) {
    lapply(l, function(ll) { ll$res })
}
timer.gettimes &lt;- function(l) {
    lapply(l, function(ll) { ll$t })
}
time.training &lt;- function(dat, algs) {
    res &lt;- lapply(algs, mktimer(mktrain(dat)))
    list(models=timer.getresults(res),
         times=timer.gettimes(res))
}
time.prediction &lt;- function(dat, models) {
    res &lt;- lapply(models, mktimer(mkconfusion(dat)))
    list(confusions=timer.getresults(res),
         times=timer.gettimes(res))
}
</code></pre>

<p>Evaluation was carried out by splitting the training set into a new training
set and test set. Each model was then trained on the training set and used
to predict the test set. The execution time of the training and prediction
tasks was recorded.</p>

<pre><code class="r">split.data &lt;- function(dat, p) {
    createDataPartition(dat[,isoutcome(dat)], list=F, p=p)    
}
getsplit.trainset &lt;- function(p) {
    sets &lt;- workingsets()
    trainset &lt;- sets[[1]]
    subtrainidx &lt;- split.data(trainset, p)
    osubtrain &lt;- trainset[subtrainidx,]
    preproc &lt;- mkpreproc(osubtrain, method=c(&quot;center&quot;, &quot;scale&quot;))
    subtrain &lt;- preproc(osubtrain)
    subtest &lt;- preproc(trainset[-subtrainidx,])
    list(training=subtrain,
         testing=subtest)
}
try.models &lt;- function(p) {
    subsets &lt;- getsplit.trainset(p)
    algs &lt;- candidate.algorithms
    train.res &lt;- time.training(subsets$training, algs)
    prediction.res &lt;- time.prediction(subsets$testing, train.res$models)
    list(train.times=train.res$times,
         models=train.res$models,
         prediction.times=prediction.res$times,
         confusions=prediction.res$confusions)
}
</code></pre>

<p>The algorithms were evaluated based on their in-sample accuracy,
out-of-sample accuracy, execution time required
for training, and execution time required for prediction.</p>

<pre><code class="r">elapsedtime &lt;- function(t) {
    t[3]
}
analyze.res &lt;- function(res) {
    data.frame(time.train=sapply(res$train.times, elapsedtime),
               time.predict=sapply(res$prediction.times, elapsedtime),
               train.accuracy=sapply(res$models, function(m) {
                   max(m$results$Accuracy) }),
               test.accuracy=sapply(res$confusions, function(cm) {
                   cm$overall[&quot;Accuracy&quot;] }),
               row.names=sapply(res$models, function(m) { m$method }))
}
</code></pre>

<h2>Evaluation Results</h2>

<p>The following table summarizes the results of this analysis when 80% of the
original training set was used to train and 20% to test. Execution times
are the elapsed wall-clock time when executed on a 2.6 GHz Intel Core i7
with 8 threads (4 physical cores and 2 threads per core). The <code>doMC</code> package
was employed to parallelize the training process where possible.</p>

<table><thead>
<tr>
<th align="left"></th>
<th align="right">time.train</th>
<th align="right">time.predict</th>
<th align="right">train.accuracy</th>
<th align="right">test.accuracy</th>
</tr>
</thead><tbody>
<tr>
<td align="left">parRF</td>
<td align="right">128.51</td>
<td align="right">0.200</td>
<td align="right">0.9925</td>
<td align="right">0.9946</td>
</tr>
<tr>
<td align="left">nnet</td>
<td align="right">48.94</td>
<td align="right">0.038</td>
<td align="right">0.7473</td>
<td align="right">0.7252</td>
</tr>
<tr>
<td align="left">nb</td>
<td align="right">122.47</td>
<td align="right">55.625</td>
<td align="right">0.7392</td>
<td align="right">0.7405</td>
</tr>
</tbody></table>

<p>The random forest performs very well here, achieving around 99% accuracy
on the training data and an estimated out-of-sample error rate of around 0.01.
The neural network and na&iuml;ve Bayesian classifier perform substantially
less well, with in-sample and out-of-sample accuracy around 70%. Their
performance is similar to one another, despite neural networks having greatly
outperformed na&iuml;ve Bayesian classifiers in the 2006 results. This may be
an indication that the <code>nnet</code> method used here is ill-suited to this dataset,
or that it requires intelligent parameter selection for good performance.</p>

<p>The neural network required the smallest amount of time both to train and to
produce predictions for the test set. The remaining algorithms took about 2.5
times as long to train.
Execution time required for prediction was negligible for the random forest and
neural network, but was around one minute for the na&iuml;ve Bayesian classifier.</p>

</body>

</html>
