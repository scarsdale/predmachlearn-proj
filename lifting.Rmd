# Classification of Weight-lifting Activity by Form

## Abstract

This report covers the application of several learning algorithms to classify
weight-lifting activity based on measurements from wearable sensors.

The dataset
was taken from an experiment performed by [Velloso et al][1], in which study
participants lifted a dumbbell either correctly or with one of four common
mistakes. Measurements were taken from sensors on the dumbbell and on the
participants' upper arm, forearm, and belt.

Three learning algorithms were evaluated on this dataset: a random forest,
a neural network, and a na&iuml;ve Bayesian classifier.

## DATA Acquisition and Preprocessing

```{r, echo=F, message=F}
library(caret)
library(knitr)
library(doMC)
registerDoMC()
download.server <- "https://d396qusza40orc.cloudfront.net"
download.basedir <- "predmachlearn"
mkurl <- function(file) {
    paste(download.server, download.basedir, file, sep="/")
}
memoize.nullary <- function(fn) {
    # memoize a function taking no arguments
    rv <- NULL
    function() {
        if (is.null(rv)) {
            rv <<- fn()
        }
        rv
    }
}
mkgetdataset <- function(filename) {
    memoize.nullary(function() {
        if (!file.exists(filename)) {
            download.file(mkurl(filename), destfile=filename, method="curl")
        }
        read.csv(filename)
    })
}
gettrain.orig <- mkgetdataset("pml-training.csv")
gettest.orig <- mkgetdataset("pml-testing.csv")
trainset.orig <- gettrain.orig()
testset.orig <- gettest.orig()
```

The dataset was acquired from a mirror, already separated into training and
test sets consisting respectively of `r nrow(trainset.orig)` and
`r nrow(testset.orig)` observations.

The complete dataset contains `r ncol(trainset.orig)` variables; this includes
the classification outcome as well as identifiers that should not be used as
predictors, such as the subject's first name. These were discarded from both
the training and test sets during pre-processing.

Among the remaining variables, many had missing values. Many of variables
with missing values were aggregate functions (like maximum and standard
deviation) taken over the entire time period during which an activity was
performed. These were removed for two reasons. First, while the original
research had used a sliding window technique to evaluate multiple
sequential observations, this analysis considers each observation independently;
indeed, the `r length(testset.orig)` observations used as the test set are
each taken from a different window and contain no aggregate values. Second,
it was believed that the aggregate functions, being derived from other
predictors, could be removed without loss of information.

Ultimately, only variables with no missing values in either the training
or test set were retained.

```{r}
mkprocdataset <- function(getfn) {
    function () {
        dat <- getfn()
        # remove columns that are not predictors
        rownames(dat) <- dat$X
        dat$X <- NULL
        dat$user_name <- NULL
        # the assignment appears to require ignoring these time series-related
        # variables and treating each observation independently
        dat$raw_timestamp_part_1 <- NULL
        dat$raw_timestamp_part_2 <- NULL
        dat$cvtd_timestamp <- NULL
        dat$new_window <- NULL
        dat$num_window <- NULL
        dat
    }
}
gettrainset <- mkprocdataset(gettrain.orig)
gettestset <- mkprocdataset(gettest.orig)
sapplycols <- function(dat, fn) {
    sapply(colnames(dat), function(colname) { fn(dat[,colname]) })
}
iscomplete <- function(v) {
    !any(is.na(v))
}
completecols <- function(dat) {
    sapplycols(dat, iscomplete)
}
mkcommonsubset <- function(gettrain, gettest) {
    function () {
        trainset <- gettrain()
        testset <- gettest()        
        completecols <- intersect(which(completecols(trainset)),
                                  which(completecols(testset)))
        list(training=trainset[,completecols],
             testing=testset[,completecols])
    }
}
workingsets <- mkcommonsubset(gettrainset, gettestset)
data <- workingsets()
```

Finally, the measurements were normalized before being used for training:

```{r}
mkpreproc <- function(dat, ...) {
    preproc <- preProcess(dat[,ispredictor(dat)], ...)
    function(dat) {
        ndat <- dat
        ndat[,ispredictor(dat)] <- predict(preproc, dat[,ispredictor(dat)])
        ndat
    }
}
```

## Learning Algorithm Evaluation

Three learning algorithms were evaluated on this dataset: a random forest,
a neural network, and a na&iuml;ve Bayesian classifier. These
algorithms were chosen based on their performance in a learning
algorithm bake-off presented by [Caruana and Niculescu-Mizil][2] in 2006.
In those results, random forests and neural networks were found to perform
very strongly on a variety of datasets, while na&iuml;ve Bayesian classifiers
were found to have very weak overall performance.

The implementations chosen were all provided by the `caret` library in R.
The `parRF` method was used for a random forest, `nnet` for a neural network,
and `nb` for a na&iuml;ve Bayesian classifier.

```{r}
candidate.algorithms <- c("parRF", "nnet", "nb")
```


```{r}
isoutcome <- function(dat) {
    colnames(dat) %in% c("classe", "problem_id")
}
ispredictor <- function(dat) {
    !isoutcome(dat)
}
mktrain <- function(dat) {
    function(algname) {
        train(dat[,ispredictor(dat)], dat[,isoutcome(dat)], method=algname)
    }
}
mkconfusion <- function(dat) {
    function(model) {
        predictions <- predict(model, dat)
        confusionMatrix(predictions, dat[,isoutcome(dat)])
    }
}
mktimer <- function(fn) {
    function(...) {
        t <- system.time(res <- fn(...))
        list(t=t, res=res)
    }
}
timer.getresults <- function(l) {
    lapply(l, function(ll) { ll$res })
}
timer.gettimes <- function(l) {
    lapply(l, function(ll) { ll$t })
}
time.training <- function(dat, algs) {
    res <- lapply(algs, mktimer(mktrain(dat)))
    list(models=timer.getresults(res),
         times=timer.gettimes(res))
}
time.prediction <- function(dat, models) {
    res <- lapply(models, mktimer(mkconfusion(dat)))
    list(confusions=timer.getresults(res),
         times=timer.gettimes(res))
}
```

Evaluation was carried out by splitting the training set into a new training
set and test set. Each model was then trained on the training set and used
to predict the test set. The execution time of the training and prediction
tasks was recorded.

```{r}
split.data <- function(dat, p) {
    createDataPartition(dat[,isoutcome(dat)], list=F, p=p)    
}
getsplit.trainset <- function(p) {
    sets <- workingsets()
    trainset <- sets[[1]]
    subtrainidx <- split.data(trainset, p)
    osubtrain <- trainset[subtrainidx,]
    preproc <- mkpreproc(osubtrain, method=c("center", "scale"))
    subtrain <- preproc(osubtrain)
    subtest <- preproc(trainset[-subtrainidx,])
    list(training=subtrain,
         testing=subtest)
}
try.models <- function(p) {
    subsets <- getsplit.trainset(p)
    algs <- candidate.algorithms
    train.res <- time.training(subsets$training, algs)
    prediction.res <- time.prediction(subsets$testing, train.res$models)
    list(train.times=train.res$times,
         models=train.res$models,
         prediction.times=prediction.res$times,
         confusions=prediction.res$confusions)
}
```

The algorithms were evaluated based on their in-sample accuracy,
out-of-sample accuracy, execution time required
for training, and execution time required for prediction.

```{r}
elapsedtime <- function(t) {
    t[3]
}
analyze.res <- function(res) {
    data.frame(time.train=sapply(res$train.times, elapsedtime),
               time.predict=sapply(res$prediction.times, elapsedtime),
               train.accuracy=sapply(res$models, function(m) {
                   max(m$results$Accuracy) }),
               test.accuracy=sapply(res$confusions, function(cm) {
                   cm$overall["Accuracy"] }),
               row.names=sapply(res$models, function(m) { m$method }))
}
```

## Evaluation Results

The following table summarizes the results of this analysis when 80% of the
original training set was used to train and 20% to test. Execution times
are the elapsed wall-clock time when executed on a 2.6 GHz Intel Core i7
with 8 threads (4 physical cores and 2 threads per core). The `doMC` package
was employed to parallelize the training process where possible.

```{r, echo=F, results="asis", cache=T}
res <- try.models(0.8)
resana <- analyze.res(res)
kable(resana, format="markdown")
```

The random forest performs very well here, achieving around 99% accuracy
on the training data and an estimated out-of-sample error rate of around 0.01.
The neural network and na&iuml;ve Bayesian classifier perform substantially
less well, with in-sample and out-of-sample accuracy around 70%. Their
performance is similar to one another, despite neural networks having greatly
outperformed na&iuml;ve Bayesian classifiers in the 2006 results. This may be
an indication that the `nnet` method used here is ill-suited to this dataset,
or that it requires intelligent parameter selection for good performance.

[1]: http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201
[2]: http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf
